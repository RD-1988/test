# Part B. Computing the language fractionalization index

1. Read the .csv file you just created with the counts of Tweets per language and country. Use this dataset to compute an index of language fractionalization at the country level using the formula in equation (1) in the paper by Alesina et al (2003). Feel free to do this in any way that you consider most efficient, either using `dplyr` or with functions and loops.

```{r}
library(readxl)
library(tidyverse)
library(countrycode)

country_index <- read_csv("country_language_distribution.csv")

# I decide to deal with the name of country before caculate the index as some origins like UK:Northern Ireland"
#"UK:Isle of Wight"  need to be aggregated together before calculation.

index_countrycode <- countrycode(c(country_index$country_info), origin = 'country.name', destination = 'genc3c')
country_index$country_info[!is.na(index_countrycode)] <- index_countrycode[!is.na(index_countrycode)]

# After analysis the na turn from contrycode, I decide to
# convert below to to UKR:

#"UK:Northern Ireland"
#"UK:Isle of Wight"
#"UK:Scotland:Island of Arran"
#"UK:Scotland:Island of Skye"
#"UK:Scotland:Isle of Mull"
#"UK:Scotland:Orkney Islands:Mainland"
#"UK:Scotland:Orkney Islands:Westray"

# convert below to NOR
#"Norway:Svalbard:Edgeoya"

country_index$country_info[is.na(index_countrycode)][-4] <- "GBR"
country_index$country_info[is.na(index_countrycode)][4] <- "NOR"

country_sum <- country_index %>% group_by(country_info) %>% summarise(SUM = sum(n))
country_index <- country_index %>% left_join(country_sum)
country_index <- country_index %>% group_by(country_info) %>% summarise(index = 1- sum((n/SUM)^2))
country_index

```

2. Compute some descriptive statistics on this data, either through tables or graphs. Which countries have the highest and lowest levels of language fractionalization?

```{r}
lowest_country <- (country_index %>%filter(index >0 ) %>% arrange(index))[1,]
highest_country <- (country_index %>%filter(index >0 ) %>% arrange(desc(index)))[1,]

```

3. Read the first sheet in `2003_fractionalization.xls` into R. Keep only the country and language fractionalization columns. (You can export the .xls file into a .csv file in Excel first if that's esier.)

Then, merge this data frame with the country-level fractionalization index you computed using Twitter data. This may be somewhat painful due to the different spellings of the countries. You can use the `countrycode` package to help you, or manually fix some of the country names so that they're the same across data sources. Throughout this process, check the sample size of the initial and final files to make sure you didn't drop any relevant countries.

```{r}



df <- read_xls("2003_fractionalization.xls", skip = 1)
df <- (df %>% select(Country, Language))[-1,]
df$Language <- as.numeric(df$Language)
df_countrycode <- countrycode(c(df$Country), origin = 'country.name', destination = 'genc3c')
index_countrycode <- countrycode(c(country_index$country_info), origin = 'country.name', destination = 'genc3c')
df$Country[!is.na(df_countrycode)] <- df_countrycode[!is.na(df_countrycode)]
df$Country[is.na(df_countrycode)] <- df$Country[is.na(df_countrycode)][1:4]
names(df) <- c("country_info", "xls_index")
my_metric <- country_index %>% left_join(df, by = "country_info")

my_metric
```


4. Compare your new metric with the measure on fractionalization from Alesina et al. What is the correlation between the two? For which countries do you find any differences? Can you explain why? Use any statistical or graphical methods you consider appropriate to answer this question.

```{r}

print(my_metric$index)
print(my_metric$xls_index)
correlation <- cor(my_metric$index, my_metric$xls_index,use = "complete.obs")
correlation

```


Save the file to disk.

```{r}

write.csv(my_metric, "my_metric.csv")

```
